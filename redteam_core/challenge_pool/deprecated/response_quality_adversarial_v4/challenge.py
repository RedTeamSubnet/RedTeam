import os
import random

import numpy as np
import openai
from data_types import MinerInput, MinerOutput
from model import ResponseQualityScoringModel
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from rouge_score import rouge_scorer

class Challenge:
    """
    A class that sets up the challenge and scores the miner's performance.
    It provides the task to be completed and evaluates the output.
    """

    def __init__(self):
        self.scoring_model = ResponseQualityScoringModel()
        VLLM_URL = os.environ.get("VLLM_URL", "http://127.0.0.1:8000/v1")
        VLLM_API_KEY = os.environ.get("VLLM_API_KEY", "api-key")
        self.model_name = os.environ.get(
            "VLLM_MODEL", "unsloth/Meta-Llama-3.1-8B-Instruct"
        )
        self.vllm_client = openai.OpenAI(
            base_url=VLLM_URL,
            api_key=VLLM_API_KEY,
        )

        self.rouge_scorer = rouge_scorer.RougeScorer(
            rouge_types=["rougeL"],
            use_stemmer=True,
        )

        with open("instructions.txt") as f:
            self.instructions = f.readlines()
        with open("toxic_instructions.txt") as f:
            self.toxic_instructions = f.readlines()

        self.stop_words = set(stopwords.words("english"))

    def prepare_task(self) -> MinerInput:
        """
        Prepares the task by returning an instance of MinerInput,
        which contains the task description.
        """
        if random.random() < 0.9:
            original_prompt = random.choice(self.instructions)
            # Rephrase non-toxic instructions
            original_prompt = self._rephrase_question(original_prompt)
        else:
            original_prompt = random.choice(self.toxic_instructions)

        modified_prompt = self._generate_modified_prompt(original_prompt)
        reference_response = self._call_vllm([{"role": "user", "content": original_prompt}])
        return MinerInput(
            original_prompt=original_prompt, modified_prompt=modified_prompt, reference_response=reference_response
        )

    def score_task(self, miner_input: MinerInput, miner_output: MinerOutput) -> float:
        """
        Evaluates the output generated by the miner.
        """
        score = self.scoring_model.get_response_quality_score(
            miner_input.original_prompt, miner_output.response, miner_input.reference_response
        )
        return score

    def compare(
        self,
        miner_input: MinerInput,
        miner_output: MinerOutput,
        reference_output: MinerOutput,
    ) -> float:
        """
        Compares two responses and returns a score between 0 and 1.
        """
        miner_response = miner_output.response
        reference_response = reference_output.response

        embedding_similarity = (
            (
                self.scoring_model.embedding_model.encode(miner_input.original_prompt)
                * self.scoring_model.embedding_model.encode(miner_input.modified_prompt)
            )
            .sum()
            .item()
        )

        rouge_score = float(
            self.rouge_scorer.score(reference_response, miner_response)[
                "rougeL"
            ].fmeasure
        )

        similarity = float((embedding_similarity + 1) / 2 + rouge_score) / 2

        return similarity

    def _rephrase_question(self, original_prompt: str) -> str:
        PROMPT_REPHRASE = f"""You are an expert. Given the following input:

Question: {original_prompt}

Paraphrase and enhance the question by making it clearer, more insightful, and adding relevant context or clues when necessary.
Ensure the improved question remains precise, relevant, and does not introduce incorrect assumptions.
Return only one modified question without any explanation.
"""
        messages = [{"role": "user", "content": PROMPT_REPHRASE}]
        try:
            response = self._call_vllm(messages)
            return response
        except Exception as e:
            print(f"[REPHRASE] Failed to rephrase the question: {e}")
            return original_prompt

    def _generate_modified_prompt(self, original_prompt: str) -> str:
        """
        Generates a modified version of the original prompt by masking a key term.
        Ensures that random sampling does not exceed the available valid words.
        """
        words = word_tokenize(original_prompt)
        stop_word_indices = [i for i, word in enumerate(words) if word.lower() in self.stop_words]

        valid_indices = [i for i in range(len(words)) if i not in stop_word_indices]
        total_words = len(valid_indices)

        num_to_mask = max(1, int(total_words * 0.1))  # Ensure at least 1 word is masked
        num_to_mask = min(num_to_mask, len(valid_indices))  # Prevent sampling error

        mask_indices = random.sample(valid_indices, num_to_mask)

        modified_prompt = " ".join("BLANK" if i in mask_indices else word for i, word in enumerate(words))

        return modified_prompt

    def _generate_reference_response(self, original_prompt: str) -> str:
        messages = [
            {
                "role": "user",
                "content": original_prompt
            }
        ]
        reference_response = self._call_vllm(messages)
        return reference_response

    def _call_vllm(self, messages):
        response = self.vllm_client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            max_tokens=2048,
        )
        print(response)
        content = response.choices[0].message.content
        return content


if __name__ == "__main__":
    challenge = Challenge()
    print(challenge.prepare_task())
